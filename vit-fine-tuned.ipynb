{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "604ed04d-934f-4097-aaca-7fb96c84d028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
    "import torchvision.models as models\n",
    "from torchinfo import summary\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "# Instantiate VisionTransformer with pretrained weights\n",
    "weights = ViT_B_16_Weights.DEFAULT  # or any other available pretrained weights\n",
    "pretrained_vit = vit_b_16(weights=weights)\n",
    "\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\"\n",
    "print(device)\n",
    "batch_size = 1024\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19bd1cac-a0bd-48bd-8ce1-de00ab7f58f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_vit = models.vit_b_16(weights=weights) # b stands for base and 16 stands for the patch size\n",
    "pretrained_vit\n",
    "\n",
    "class_names = [\"pizza\", \"steak\", \"sushi\"]\n",
    "# key idea is to freeze layers in pretrained vit model and only train the last layer of the head\n",
    "\n",
    "for param in pretrained_vit.parameters():\n",
    "    param.requires_grad = False # this is where we freeze the layers\n",
    "    \n",
    "    embedding_dim = 768 \n",
    "    # update the pretrained head\n",
    "    pretrained_vit.heads = nn.Sequential(\n",
    "        nn.LayerNorm(normalized_shape=embedding_dim),\n",
    "        nn.Linear(in_features=embedding_dim, out_features=len(class_names)) # we change out_features from 1000 to 3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "131591f8-8e45-454a-bbc3-bfc150128d30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
       "============================================================================================================================================\n",
       "VisionTransformer (VisionTransformer)                        [1, 3, 224, 224]     [1, 3]               768                  Partial\n",
       "├─Conv2d (conv_proj)                                         [1, 3, 224, 224]     [1, 768, 14, 14]     (590,592)            False\n",
       "├─Encoder (encoder)                                          [1, 197, 768]        [1, 197, 768]        151,296              False\n",
       "│    └─Dropout (dropout)                                     [1, 197, 768]        [1, 197, 768]        --                   --\n",
       "│    └─Sequential (layers)                                   [1, 197, 768]        [1, 197, 768]        --                   False\n",
       "│    │    └─EncoderBlock (encoder_layer_0)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_1)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_2)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_3)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_4)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_5)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_6)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_7)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_8)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_9)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_10)                  [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_11)                  [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    └─LayerNorm (ln)                                        [1, 197, 768]        [1, 197, 768]        (1,536)              False\n",
       "├─Sequential (heads)                                         [1, 768]             [1, 3]               --                   True\n",
       "│    └─LayerNorm (0)                                         [1, 768]             [1, 768]             1,536                True\n",
       "│    └─Linear (1)                                            [1, 768]             [1, 3]               2,307                True\n",
       "============================================================================================================================================\n",
       "Total params: 85,802,499\n",
       "Trainable params: 3,843\n",
       "Non-trainable params: 85,798,656\n",
       "Total mult-adds (M): 172.47\n",
       "============================================================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 104.09\n",
       "Params size (MB): 229.21\n",
       "Estimated Total Size (MB): 333.90\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summary \n",
    "summary(model=pretrained_vit, \n",
    "        input_size=(1, 3, 224, 224), \n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20, \n",
    "        row_settings=[\"var_names\"]\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8f358db-edbc-435c-8d11-43cd065d4b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import wget\n",
    "# import zipfile\n",
    "\n",
    "# # Create a directory to store the downloaded data\n",
    "# data_directory = \"pizza_steak_sushi\"\n",
    "# os.makedirs(data_directory, exist_ok=True)\n",
    "\n",
    "# # Download the ZIP file\n",
    "# url = \"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\"\n",
    "# zip_file = os.path.join(data_directory, \"pizza_steak_sushi_20_percent.zip\")\n",
    "# wget.download(url, zip_file)\n",
    "\n",
    "# # Extract the contents of the ZIP file\n",
    "# with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "#     zip_ref.extractall(data_directory)\n",
    "\n",
    "\n",
    "# # Remove the ZIP file\n",
    "# os.remove(zip_file)\n",
    "\n",
    "# # Print the paths\n",
    "# print(\"Train directory:\", train_dir_20_percent)\n",
    "# print(\"Test directory:\", test_dir_20_percent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99587b74-e8f3-436d-bbf0-22035b5ba7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Set up train/test directories\n",
    "data_directory = \"pizza_steak_sushi\"\n",
    "train_dir_20_percent = os.path.join(data_directory, \"train\")\n",
    "test_dir = os.path.join(data_directory, \"test\")\n",
    "\n",
    "# Define data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to a specific size\n",
    "    transforms.ToTensor(),          # Convert images to tensors\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize pixel values\n",
    "])\n",
    "\n",
    "# Create train data loader\n",
    "train_dataset = datasets.ImageFolder(train_dir_20_percent, transform=transform)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create test data loader\n",
    "test_dataset = datasets.ImageFolder(test_dir, transform=transform)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Get class names\n",
    "class_names = train_dataset.classes                                                        \n",
    "                                                         \n",
    "print(len(train_dataloader))\n",
    "print(len(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c051ca-b07f-4d9f-b686-bd8861e7552b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2715d251-1c6d-437a-a7a7-e4b659435353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa82b48-2adb-412d-a37c-c0bbc96f4959",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bff5b6d-9627-420c-9014-19e76c19b120",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
