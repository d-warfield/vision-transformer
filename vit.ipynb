{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a690ba15-fbff-4f9c-b047-d4dba58c2b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c404f6a6-cfd8-41ea-bdf9-ee96bf508ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePatcher(nn.Module):\n",
    "    def __init__(self, image_channels, patch_size, embedding_dimension):\n",
    "        '''\n",
    "        Initialize ImagePatcher class.\n",
    "\n",
    "        This helper class creates patches from the input image and transforms them into embeddings.\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        '''\n",
    "        Initialize a convolutional layer with the number of output channels equal to the embedding dimension, \n",
    "        and kernel size and stride both equal to the patch size. This layer helps in breaking the image into patches \n",
    "        and transforming each patch into embeddings.\n",
    "        '''\n",
    "        self.conv = nn.Conv2d(image_channels, embedding_dimension, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "        '''\n",
    "        Flatten the 2D patches created by the convolutional layer into 1D vectors. \n",
    "        The flattening starts from the 2nd dimension to the 3rd.\n",
    "        '''\n",
    "        self.flatten = nn.Flatten(start_dim=2, end_dim=3)\n",
    "\n",
    "    def forward(self, input_image):\n",
    "        '''\n",
    "        Forward propagation for the ImagePatcher module. \n",
    "\n",
    "        The input image is passed through the convolutional layer to create patches, which are then flattened.\n",
    "        The output tensor is then permuted to get the right shape for the transformer.\n",
    "        '''\n",
    "        return self.flatten(self.conv(input_image)).permute(0, 2, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63623e1e-f1af-4c0a-9a0b-c16b12575b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module): \n",
    "    def __init__(self, \n",
    "                 num_classes,\n",
    "                 image_size=224, \n",
    "                 image_channels=3, \n",
    "                 patch_size=16, \n",
    "                 embed_dimension=768, \n",
    "                 dropout=0.1, \n",
    "                 mlp_size=3072, \n",
    "                 num_transformer_layers=12, \n",
    "                 num_heads=12\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        '''\n",
    "        Ensure that the image_size is divisible by patch_size. This is required to ensure that the image can be \n",
    "        broken down into evenly sized patches.\n",
    "        '''\n",
    "        assert image_size % patch_size == 0, \"image_size must be divisible by patch_size\"\n",
    "\n",
    "        '''\n",
    "        Initialize the ImagePatcher module which extracts image patches and projects them into the embedding space. \n",
    "        This module takes the number of image channels, the patch size and the embedding dimension as input.\n",
    "        '''\n",
    "        self.patch_embedding = ImagePatcher(image_channels, patch_size, embed_dimension)\n",
    "\n",
    "        '''\n",
    "        Define the class token which is used as the first token of the sequence. The class token is initialized with\n",
    "        a random tensor that has the shape (1, 1, embed_dimension) and requires gradient.\n",
    "        '''\n",
    "        self.class_token = nn.Parameter(torch.randn(1, 1, embed_dimension), requires_grad=True)\n",
    "\n",
    "        '''\n",
    "        Compute the total number of patches by dividing the image size by the patch size and squaring the result. \n",
    "        The total number of patches represents the number of image patches we can extract from the input image.\n",
    "        '''\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "\n",
    "        '''\n",
    "        Define the positional embeddings that will be added to the patch embeddings. These embeddings represent the \n",
    "        relative or absolute position of the patches in the input image.\n",
    "        '''\n",
    "        self.positional_embedding = nn.Parameter(torch.randn(1, num_patches + 1, embed_dimension), requires_grad=True)\n",
    "\n",
    "        '''\n",
    "        Define a dropout layer with the provided dropout rate. Dropout is a regularization technique for reducing overfitting.\n",
    "        '''\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        '''\n",
    "        Initialize the transformer encoder. The transformer consists of multiple layers of multi-head self-attention \n",
    "        and feed-forward neural networks. The size of the feed-forward neural networks, the number of attention heads \n",
    "        and the number of layers are configurable.\n",
    "        '''\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                embed_dimension, num_heads, mlp_size, batch_first=True, norm_first=True\n",
    "            ), num_transformer_layers\n",
    "        )\n",
    "\n",
    "        '''\n",
    "        Define the final classification head, which is a simple multi-layer perceptron (MLP) that maps the \n",
    "        transformer output to the final class logits.\n",
    "        '''\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dimension),\n",
    "            nn.Linear(embed_dimension, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Determine the batch size from the shape of the input tensor.\n",
    "        '''\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        '''\n",
    "        Apply the patch embedding module to the input image to get the patch embeddings.\n",
    "        '''\n",
    "        x = self.patch_embedding(x)\n",
    "\n",
    "        '''\n",
    "        Add the class token at the beginning of each sequence in the batch.\n",
    "        '''\n",
    "        x = torch.cat((self.class_token.expand(batch_size, -1, -1), x), dim=1)\n",
    "\n",
    "        '''\n",
    "        Add the positional embeddings to the patch embeddings.\n",
    "        '''\n",
    "        x += self.positional_embedding\n",
    "\n",
    "        '''\n",
    "        Apply the dropout layer to the embeddings.\n",
    "        '''\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        '''\n",
    "        Feed the sequence of embeddings into the transformer.\n",
    "        '''\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        '''\n",
    "        Take the first token (class token) of each sequence, feed it through the classification head to obtain \n",
    "        the final class probabilities.\n",
    "        '''\n",
    "        return self.mlp_head(x[:, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96a6e77e-3d30-4c53-8dcd-80196fa1e51c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3264, -0.9058,  0.1786],\n",
       "        [-0.1934, -0.7492, -0.0589],\n",
       "        [-0.4165, -0.9355, -0.1744],\n",
       "        [-0.0490, -0.6681,  0.1470],\n",
       "        [-0.1315, -0.7534, -0.1728],\n",
       "        [-0.3976, -1.0031, -0.3422],\n",
       "        [-0.4002, -0.7935,  0.2675],\n",
       "        [-0.1943, -1.1838, -0.1940],\n",
       "        [-0.5635, -0.4801, -0.1250],\n",
       "        [-0.5193, -0.8771, -0.0634],\n",
       "        [-0.2233, -0.9906,  0.1719],\n",
       "        [-0.3281, -1.1864,  0.1244],\n",
       "        [-0.3698, -0.6208, -0.1982],\n",
       "        [-0.4077, -0.7038,  0.2094],\n",
       "        [-0.5601, -0.8584, -0.0521],\n",
       "        [-0.3592, -0.7675, -0.1853],\n",
       "        [-0.3318, -0.8554,  0.0860],\n",
       "        [-0.3778, -0.6205, -0.2744],\n",
       "        [-0.0699, -1.1418, -0.1092],\n",
       "        [-0.4991, -0.7855, -0.0435],\n",
       "        [-0.2633, -0.9802,  0.0239],\n",
       "        [-0.3655, -1.0466, -0.0472],\n",
       "        [-0.3331, -0.7122, -0.1524],\n",
       "        [-0.3538, -0.9178,  0.0150],\n",
       "        [-0.4726, -0.9563, -0.1842],\n",
       "        [-0.4875, -0.9477, -0.1244],\n",
       "        [-0.2309, -0.8826, -0.3113],\n",
       "        [-0.1018, -0.7241, -0.1371],\n",
       "        [-0.3035, -0.9136,  0.2212],\n",
       "        [ 0.0638, -0.6998, -0.0796],\n",
       "        [-0.1335, -0.8302,  0.1130],\n",
       "        [ 0.0020, -0.7578,  0.0538]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = [\"tiger\", \"bee\", \"dog\"]\n",
    "\n",
    "demo_images = torch.randn(32, 3, 224, 224)\n",
    "\n",
    "\n",
    "\n",
    "vit = VisionTransformer(num_classes=len(class_names))\n",
    "vit(demo_images)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02c6aa07-7d11-4568-bf92-d2e42178751c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "VisionTransformer                             [32, 3]                   152,064\n",
       "├─ImagePatcher: 1-1                           [32, 196, 768]            --\n",
       "│    └─Conv2d: 2-1                            [32, 768, 14, 14]         590,592\n",
       "│    └─Flatten: 2-2                           [32, 768, 196]            --\n",
       "├─Dropout: 1-2                                [32, 197, 768]            --\n",
       "├─TransformerEncoder: 1-3                     [32, 197, 768]            --\n",
       "│    └─ModuleList: 2-3                        --                        --\n",
       "│    │    └─TransformerEncoderLayer: 3-1      [32, 197, 768]            7,087,872\n",
       "│    │    └─TransformerEncoderLayer: 3-2      [32, 197, 768]            7,087,872\n",
       "│    │    └─TransformerEncoderLayer: 3-3      [32, 197, 768]            7,087,872\n",
       "│    │    └─TransformerEncoderLayer: 3-4      [32, 197, 768]            7,087,872\n",
       "│    │    └─TransformerEncoderLayer: 3-5      [32, 197, 768]            7,087,872\n",
       "│    │    └─TransformerEncoderLayer: 3-6      [32, 197, 768]            7,087,872\n",
       "│    │    └─TransformerEncoderLayer: 3-7      [32, 197, 768]            7,087,872\n",
       "│    │    └─TransformerEncoderLayer: 3-8      [32, 197, 768]            7,087,872\n",
       "│    │    └─TransformerEncoderLayer: 3-9      [32, 197, 768]            7,087,872\n",
       "│    │    └─TransformerEncoderLayer: 3-10     [32, 197, 768]            7,087,872\n",
       "│    │    └─TransformerEncoderLayer: 3-11     [32, 197, 768]            7,087,872\n",
       "│    │    └─TransformerEncoderLayer: 3-12     [32, 197, 768]            7,087,872\n",
       "├─Sequential: 1-4                             [32, 3]                   --\n",
       "│    └─LayerNorm: 2-4                         [32, 768]                 1,536\n",
       "│    └─Linear: 2-5                            [32, 3]                   2,307\n",
       "===============================================================================================\n",
       "Total params: 85,800,963\n",
       "Trainable params: 85,800,963\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 3.70\n",
       "===============================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 38.73\n",
       "Params size (MB): 2.38\n",
       "Estimated Total Size (MB): 60.38\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model=vit, input_size=demo_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8068757-946f-4b95-af01-6d0ba932b9b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
