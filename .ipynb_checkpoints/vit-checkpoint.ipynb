{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf108d53-8226-480b-b867-2e93c4bbdbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb8a5f57-4216-4fb9-833f-bc11f447e76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python class is used to segment images into patches and convert them into embeddings.\n",
    "class ImagePatchEmbedding(nn.Module):\n",
    "\n",
    "    # The constructor initializes the class instance with image size, patch size, image channels, and the embedding dimension.\n",
    "    def __init__(self, image_size, patch_size, image_channels, embedding_dimension=768):\n",
    "        \n",
    "        # Invoking the parent class's constructor\n",
    "        super().__init__()\n",
    "\n",
    "        # Assigning input parameters to class instance variables\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.number_of_patches = (image_size // patch_size) ** 2 # Here we calculate the total number of patches\n",
    "        \n",
    "        # We use a convolutional layer to segment our image into patches.\n",
    "        # The convolutional layer will have a kernel size and stride equal to the patch size.\n",
    "        self.conv_layer_to_segment_image = nn.Conv2d(in_channels=image_channels, out_channels=embedding_dimension, kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "        # A layer to transform the 2D patch feature maps into 1D vectors\n",
    "        self.flatten_layer = nn.Flatten(start_dim=2, end_dim=3)\n",
    "        \n",
    "    # This method processes the image through our convolutional layer to generate the patches and convert them into embeddings.\n",
    "    def forward(self, input_image):\n",
    "        \n",
    "        # Check that the size of the input image is divisible by the patch size\n",
    "        image_resolution = input_image.shape[-1]\n",
    "        assert image_resolution % self.patch_size == 0, \"Input image size must be divisible by the patch size.\"\n",
    "        \n",
    "        # Segmenting the image into patches\n",
    "        patches = self.conv_layer_to_segment_image(input_image)\n",
    "        \n",
    "        # Flatten the patches into 1D vectors\n",
    "        flattened_patches = self.flatten_layer(patches)\n",
    "        \n",
    "        # Reorder the dimensions to have the batch size, number of patches and embedding dimension.\n",
    "        reshaped_patches = flattened_patches.permute(0, 2, 1)\n",
    "        \n",
    "        return reshaped_patches\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b98b550a-67b0-47a3-a683-4da9e234acc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 3, 224, 224])\n",
      "Output shape: torch.Size([32, 196, 768]) -> (batch_size, num_patches, embedding_dim)\n"
     ]
    }
   ],
   "source": [
    "rand_image_tensor = torch.randn(32, 3, 224, 224)  # (batch_size, color_channels, height, width)\n",
    "rand_image_tensor.shape\n",
    "\n",
    "patch_embedding = ImagePatchEmbedding(224, 16, 3)\n",
    "patch_embedding_output = patch_embedding.forward(rand_image_tensor)\n",
    "print(f\"Input shape: {rand_image_tensor.shape}\")\n",
    "print(f\"Output shape: {patch_embedding_output.shape} -> (batch_size, num_patches, embedding_dim)\")\n",
    "\n",
    "# To summarize, you have 32 batches, each batch contains 196 patches, and each patch has a dimension of 764.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "646dc08f-7fb6-43d4-a849-2dbb4a453672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoderLayer(\n",
       "  (self_attn): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "  (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout1): Dropout(p=0.1, inplace=False)\n",
       "  (dropout2): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=768,\n",
    "                                                       nhead=12,\n",
    "                                                       dim_feedforward=3072,\n",
    "                                                       dropout=0.1,\n",
    "                                                       activation=\"gelu\",\n",
    "                                                       batch_first=True,\n",
    "                                                       norm_first=True)\n",
    "transformer_encoder_layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e548ac1-e1d2-48d7-a409-32e868fe74a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(model=transformer_encoder_layer, input_size=patch_embedding_output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e5ee9ed3-acb4-48e5-b481-a582fa4e5fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "TransformerEncoder                       [32, 196, 768]            --\n",
       "├─ModuleList: 1-1                        --                        --\n",
       "│    └─TransformerEncoderLayer: 2-1      [32, 196, 768]            7,087,872\n",
       "│    └─TransformerEncoderLayer: 2-2      [32, 196, 768]            7,087,872\n",
       "│    └─TransformerEncoderLayer: 2-3      [32, 196, 768]            7,087,872\n",
       "│    └─TransformerEncoderLayer: 2-4      [32, 196, 768]            7,087,872\n",
       "│    └─TransformerEncoderLayer: 2-5      [32, 196, 768]            7,087,872\n",
       "│    └─TransformerEncoderLayer: 2-6      [32, 196, 768]            7,087,872\n",
       "│    └─TransformerEncoderLayer: 2-7      [32, 196, 768]            7,087,872\n",
       "│    └─TransformerEncoderLayer: 2-8      [32, 196, 768]            7,087,872\n",
       "│    └─TransformerEncoderLayer: 2-9      [32, 196, 768]            7,087,872\n",
       "│    └─TransformerEncoderLayer: 2-10     [32, 196, 768]            7,087,872\n",
       "│    └─TransformerEncoderLayer: 2-11     [32, 196, 768]            7,087,872\n",
       "│    └─TransformerEncoderLayer: 2-12     [32, 196, 768]            7,087,872\n",
       "==========================================================================================\n",
       "Total params: 85,054,464\n",
       "Trainable params: 85,054,464\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0\n",
       "==========================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 19.27\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we want to stack 12 transformer encoders layers to make up the entire transformer encoder block\n",
    "\n",
    "# an encoder says \"I'll turn your data into a numerical representation to try and find a pattern in that data\"\n",
    "# a decoder says \"I'll take your learnable numerical representation and turn it back to human understandable\"\n",
    "\n",
    "transformer_encoder = nn.TransformerEncoder(\n",
    "    encoder_layer=transformer_encoder_layer,\n",
    "    num_layers=12\n",
    ")\n",
    "\n",
    "\n",
    "summary(model=transformer_encoder, input_size=patch_embedding_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "301f4c40-4c5b-47ed-b041-5078e93ccf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put it all together and create ViT\n",
    "class VisionTransformer(nn.Module): # we want to subclass the VisionTransformer class\n",
    "    def __init__(self, \n",
    "                 num_classes,\n",
    "                 image_size=224, \n",
    "                 image_channels=3, \n",
    "                 patch_size=16, \n",
    "                 embed_dimension=768, \n",
    "                 dropout=0.1, \n",
    "                 mlp_size=3072, \n",
    "                 num_transformer_layers=12, \n",
    "                 num_heads=12, # number of multi-head self attention heads\n",
    "                 ): # 1,000 for the the labels say imageNet\n",
    "        super().__init__() # initialize the parent classes nn.Module's contructor before adding any intialization logic to VisionTransformer class\n",
    "    \n",
    "        assert image_size % patch_size == 0, f\"image_size must divisible by patch_size\"\n",
    "    \n",
    "    \n",
    "    \n",
    "        # 1. Create patch embedding\n",
    "        self.patch_embedding = ImagePatchEmbedding(image_size=image_size, patch_size=patch_size, image_channels=image_channels) # remember, we create embeddings to form a representation of our data so it can be learned and updated over time\n",
    "        \n",
    "        # 2. Create class token for a single image\n",
    "        self.class_token = nn.Parameter(torch.randn(1, 1, embed_dimension), requires_grad=True) # requires_grad means that the parameter is \"learnable\"\n",
    "        \n",
    "        # 3. Create positional embedding (\"this is the order that the patches come in\" -> keeps track of where the patches are positioned in a sequence because the spatial information is lost when it's flattened into a sequence)\n",
    "        num_patches = (image_size * image_size) // patch_size**2 # N = HW / P^2\n",
    "        self.positional_embedding = nn.Parameter(torch.randn(1, num_patches + 1, embed_dimension), requires_grad=True) # create positional embed for each patch in image and also the class token\n",
    "        \n",
    "        # 4. Create patch + position embedding dropout\n",
    "        self.embed_dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # 5. Create transformer encoder layer\n",
    "        self.transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dimension, \n",
    "                                                                    nhead=num_heads, \n",
    "                                                                    dim_feedforward=mlp_size, \n",
    "                                                                    activation=\"gelu\", \n",
    "                                                                    batch_first=True, \n",
    "                                                                    norm_first=True)\n",
    "        \n",
    "        \n",
    "        # 6. Create stack transformer encoder layers\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer=self.transformer_encoder_layer, num_layers=num_transformer_layers  )\n",
    "        \n",
    "        \n",
    "        # 7. Create MLP head\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(normalized_shape=embed_dimension),\n",
    "            nn.Linear(in_features=embed_dimension, out_features=num_classes)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x): # \"Hey, what do you want me to do?\" If I pass you some data in the form of 'x', I want you to take these steps\n",
    "        # dims from x\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        # create patch embedding\n",
    "        x = self.patch_embedding(x)\n",
    "\n",
    "        # Expand the class token across the batch dimension\n",
    "        class_token = self.class_token.expand(batch_size, -1, -1) # copy class token 32 times or the size of the batch and then use -1, to infer the rest of the dimensions\n",
    "        print(f\"Shape of image with class token {x.shape}, it's 197 now instead of 196\")\n",
    "\n",
    "\n",
    "        # Prepend the class token to the patch embeddings\n",
    "        x = torch.cat((class_token, x), dim=1) # Why dim1, we want to prepend it to our patches dim (batch_size, patches, embed_dim) from the patch_embedding class\n",
    "\n",
    "        # Add positional embeddings to class token and patch embeddings\n",
    "        x = self.positional_embedding + x\n",
    "        print(f\"pos + (class token, patch emd) {x.shape}\")\n",
    "        \n",
    "        \n",
    "        # Dropout on patch + positional embeddings\n",
    "        x = self.embed_dropout(x)\n",
    "        \n",
    "        # Pass embedding through transformer encoder stack\n",
    "        x = self.transformer_encoder(x)\n",
    "        \n",
    "        # Pass 0th index of x through MLP head, why? Only pass class token to the MLP head for classification\n",
    "        x = self.mlp_head(x[:, 0]) # : means all of the batches, but only the 0th dim\n",
    "        \n",
    "        \n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f632749-4b2f-4ce3-bc91-6ae2020a22b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of image with class token torch.Size([32, 196, 768]), it's 197 now instead of 196\n",
      "pos + (class token, patch emd) torch.Size([32, 197, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nInput Image Shape: torch.Size([32, 3, 224, 224]) | 32 = Batch Size | 3 = Image Channels | 224 = Image Height | 224 = Image Width\\nPatched Shape: torch.Size([32, 768, 14, 14]) | 32 = Batch Size | 768 = Embed Dimension | 14 = Patch Height | 14 = Patch Width\\nFlattened Shape: torch.Size([32, 768, 196]) | 32 = Batch Size | 768 = Embed Dimension | 196 = Flattened Patches\\nPermuted Shape: torch.Size([32, 196, 768]) | 32 = Batch Size | 196 = Flattened Patches | 768 = Embed Dimension\\nShape of image with class token torch.Size([32, 196, 768]), it's 197 now instead of 196\\npos + (class token, patch emd) torch.Size([32, 197, 768])\\ntensor([[ 0.4577,  0.4281, -0.0289],\\n        [ 0.3021,  0.6390, -0.2092],\\n        [ 0.3056,  0.6422,  0.0428],\\n        [ 0.2205,  0.6570, -0.1065],\\n        [ 0.0620,  0.9290, -0.3605],\\n        [ 0.3369,  0.7926, -0.0963],\\n        [ 0.2043,  0.4202,  0.2618],\\n        [ 0.0527,  0.5007, -0.1787],\\n        [ 0.0195,  0.6088, -0.0205],\\n        [ 0.0513,  0.6730, -0.1341],\\n        [ 0.1731,  0.7196, -0.0676],\\n        [ 0.0735,  0.5199, -0.1979],\\n        [ 0.2313,  0.3570, -0.0364],\\n        [ 0.2900,  0.6627, -0.1341],\\n        [ 0.0354,  0.8942, -0.2055],\\n        [ 0.1819,  0.6816, -0.1065],\\n        [ 0.0393,  0.4170, -0.1408],\\n        [-0.0083,  0.5982, -0.0175],\\n        [ 0.1635,  0.6088, -0.0194],\\n        [ 0.1536,  0.0958, -0.0981],\\n        [ 0.1868,  0.1048,  0.0895],\\n        [-0.1212,  0.5534,  0.1458],\\n        [ 0.4610,  0.6150, -0.1061],\\n        [ 0.4095,  0.6661, -0.1383],\\n        [ 0.3896,  0.7688,  0.0430],\\n        [ 0.0205,  0.5659,  0.0815],\\n        [ 0.3623,  0.5507, -0.1554],\\n        [ 0.4797,  0.7323, -0.0357],\\n        [ 0.2438,  0.4421, -0.0457],\\n        [ 0.3576,  0.2898, -0.0273],\\n        [-0.0353,  0.5208, -0.2585],\\n        [ 0.1440,  0.7174,  0.1166]], grad_fn=<AddmmBackward0>)\\n\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = [\"apple\", \"car\", \"dog\"]\n",
    "\n",
    "demo_images = torch.randn(32, 3, 224, 224)\n",
    "vit = VisionTransformer(num_classes=len(class_names))\n",
    "vit(demo_images)\n",
    "\n",
    "\n",
    "'''\n",
    "Input Image Shape: torch.Size([32, 3, 224, 224]) | 32 = Batch Size | 3 = Image Channels | 224 = Image Height | 224 = Image Width\n",
    "Patched Shape: torch.Size([32, 768, 14, 14]) | 32 = Batch Size | 768 = Embed Dimension | 14 = Patch Height | 14 = Patch Width\n",
    "Flattened Shape: torch.Size([32, 768, 196]) | 32 = Batch Size | 768 = Embed Dimension | 196 = Flattened Patches\n",
    "Permuted Shape: torch.Size([32, 196, 768]) | 32 = Batch Size | 196 = Flattened Patches | 768 = Embed Dimension\n",
    "Shape of image with class token torch.Size([32, 196, 768]), it's 197 now instead of 196\n",
    "pos + (class token, patch emd) torch.Size([32, 197, 768])\n",
    "tensor([[ 0.4577,  0.4281, -0.0289],\n",
    "        [ 0.3021,  0.6390, -0.2092],\n",
    "        [ 0.3056,  0.6422,  0.0428],\n",
    "        [ 0.2205,  0.6570, -0.1065],\n",
    "        [ 0.0620,  0.9290, -0.3605],\n",
    "        [ 0.3369,  0.7926, -0.0963],\n",
    "        [ 0.2043,  0.4202,  0.2618],\n",
    "        [ 0.0527,  0.5007, -0.1787],\n",
    "        [ 0.0195,  0.6088, -0.0205],\n",
    "        [ 0.0513,  0.6730, -0.1341],\n",
    "        [ 0.1731,  0.7196, -0.0676],\n",
    "        [ 0.0735,  0.5199, -0.1979],\n",
    "        [ 0.2313,  0.3570, -0.0364],\n",
    "        [ 0.2900,  0.6627, -0.1341],\n",
    "        [ 0.0354,  0.8942, -0.2055],\n",
    "        [ 0.1819,  0.6816, -0.1065],\n",
    "        [ 0.0393,  0.4170, -0.1408],\n",
    "        [-0.0083,  0.5982, -0.0175],\n",
    "        [ 0.1635,  0.6088, -0.0194],\n",
    "        [ 0.1536,  0.0958, -0.0981],\n",
    "        [ 0.1868,  0.1048,  0.0895],\n",
    "        [-0.1212,  0.5534,  0.1458],\n",
    "        [ 0.4610,  0.6150, -0.1061],\n",
    "        [ 0.4095,  0.6661, -0.1383],\n",
    "        [ 0.3896,  0.7688,  0.0430],\n",
    "        [ 0.0205,  0.5659,  0.0815],\n",
    "        [ 0.3623,  0.5507, -0.1554],\n",
    "        [ 0.4797,  0.7323, -0.0357],\n",
    "        [ 0.2438,  0.4421, -0.0457],\n",
    "        [ 0.3576,  0.2898, -0.0273],\n",
    "        [-0.0353,  0.5208, -0.2585],\n",
    "        [ 0.1440,  0.7174,  0.1166]], grad_fn=<AddmmBackward0>)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e024b02a-6fc5-4e73-9d0b-bce3571efad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model=vit, input_size=demo_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884375bd-c3b2-406f-9a2e-f9af56fc084c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a690ba15-fbff-4f9c-b047-d4dba58c2b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3584, -0.3832,  0.3969],\n",
       "        [ 0.1811, -0.7862,  0.4041],\n",
       "        [ 0.2111, -0.3494,  0.1924],\n",
       "        [ 0.3677, -0.4780,  0.2955],\n",
       "        [ 0.1036, -0.3134,  0.0314],\n",
       "        [ 0.6630, -0.4446,  0.5569],\n",
       "        [ 0.3351, -0.5870,  0.5440],\n",
       "        [ 0.1930, -0.4504,  0.4822],\n",
       "        [ 0.0944, -0.7169,  0.0551],\n",
       "        [ 0.1774, -0.7897,  0.2435],\n",
       "        [ 0.1660, -0.4201,  0.0363],\n",
       "        [ 0.2953, -0.3338,  0.3320],\n",
       "        [ 0.4666, -0.3435,  0.2051],\n",
       "        [ 0.2962, -0.4594,  0.2008],\n",
       "        [ 0.2610, -0.5089,  0.4607],\n",
       "        [ 0.2954, -0.5357,  0.3910],\n",
       "        [ 0.4862, -0.1919,  0.0420],\n",
       "        [ 0.2963, -0.4240,  0.2787],\n",
       "        [ 0.2013, -0.4501,  0.5937],\n",
       "        [ 0.3435, -0.2963,  0.4042],\n",
       "        [ 0.4576, -0.4095,  0.1598],\n",
       "        [ 0.5392, -0.5060,  0.2855],\n",
       "        [ 0.1561, -0.3194,  0.3333],\n",
       "        [ 0.1777, -0.4671,  0.2991],\n",
       "        [ 0.2925, -0.2399,  0.0908],\n",
       "        [ 0.3274, -0.4938,  0.3047],\n",
       "        [ 0.2222, -0.5913,  0.1426],\n",
       "        [ 0.2622, -0.3481,  0.3127],\n",
       "        [ 0.5972, -0.3444,  0.2685],\n",
       "        [ 0.4045, -0.4962,  0.1982],\n",
       "        [ 0.2660, -0.3429,  0.2692],\n",
       "        [ 0.2481, -0.4575,  0.4666]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c404f6a6-cfd8-41ea-bdf9-ee96bf508ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This is a helper class for creating patches from the input image.\n",
    "class ImagePatcher(nn.Module):\n",
    "    def __init__(self, image_channels, patch_size, embedding_dimension):\n",
    "        super().__init__()\n",
    "        # This convolutional layer helps in breaking the image into patches and transforming each patch into embeddings.\n",
    "        self.conv = nn.Conv2d(image_channels, embedding_dimension, kernel_size=patch_size, stride=patch_size)\n",
    "        # Flatten the 2D patches into 1D vectors.\n",
    "        self.flatten = nn.Flatten(start_dim=2, end_dim=3)\n",
    "\n",
    "    # Forward propagation for the image patcher. \n",
    "    def forward(self, input_image):\n",
    "        # The image is passed through the convolution layer to create patches and then flattened.\n",
    "        # The output tensor is then permuted to get the right shape for the transformer.\n",
    "        return self.flatten(self.conv(input_image)).permute(0, 2, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63623e1e-f1af-4c0a-9a0b-c16b12575b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This is the main Vision Transformer class.\n",
    "class VisionTransformer(nn.Module): \n",
    "    def __init__(self, \n",
    "                 num_classes,\n",
    "                 image_size=224, \n",
    "                 image_channels=3, \n",
    "                 patch_size=16, \n",
    "                 embed_dimension=768, \n",
    "                 dropout=0.1, \n",
    "                 mlp_size=3072, \n",
    "                 num_transformer_layers=12, \n",
    "                 num_heads=12,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        # Checking if image_size is divisible by patch_size.\n",
    "        assert image_size % patch_size == 0, f\"image_size must be divisible by patch_size\"\n",
    "\n",
    "        # Initialize the ImagePatcher module.\n",
    "        self.patch_embedding = ImagePatcher(image_channels, patch_size, embed_dimension)\n",
    "        # Initialize the class token which is used as the first token in the sequence.\n",
    "        self.class_token = nn.Parameter(torch.randn(1, 1, embed_dimension), requires_grad=True)\n",
    "        # Calculate the number of patches.\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        # Initialize the positional embeddings for each patch and the class token.\n",
    "        self.positional_embedding = nn.Parameter(torch.randn(1, num_patches + 1, embed_dimension), requires_grad=True)\n",
    "        # Initialize the dropout layer.\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Initialize the transformer encoder with the specified dimensions and layers.\n",
    "        self.transformer = nn.TransformerEncoder(nn.TransformerEncoderLayer(embed_dimension, num_heads, mlp_size, batch_first=True, norm_first=True), num_transformer_layers)\n",
    "        # Initialize the final classification head which consists of layer normalization and a linear layer.\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dimension),\n",
    "            nn.Linear(embed_dimension, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Determine the batch size.\n",
    "        batch_size = x.shape[0]\n",
    "        # Create patches from the input image and convert them into embeddings.\n",
    "        x = self.patch_embedding(x)\n",
    "        # Add the class token to the beginning of each sequence.\n",
    "        x = torch.cat((self.class_token.expand(batch_size, -1, -1), x), dim=1)\n",
    "        # Add the positional embeddings to the patch embeddings.\n",
    "        x += self.positional_embedding\n",
    "        # Apply dropout.\n",
    "        x = self.dropout(x)\n",
    "        # Pass the sequence through the transformer.\n",
    "        x = self.transformer(x)\n",
    "        # Pass the first token (class token) through the final classification head to get the output probabilities for each class.\n",
    "        return self.mlp_head(x[:, 0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
